{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76031b72",
   "metadata": {},
   "source": [
    "# Projet Traitement de données multidimensionnelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c38824",
   "metadata": {},
   "source": [
    "Le projet vise à créer une intelligence artificielle (IA) utilisant un Large Language Model (LLM) pour catégoriser automatiquement les commentaires sur des séries télévisées. L'approche détaillée pour réaliser ce projet :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a235dd",
   "metadata": {},
   "source": [
    "1. Choixd'un LLM adéquat\n",
    "Il y a plusieursmodèles comme BERT, GPT-2 ..., chaque modèle a ses forces spécifiques pour différentes tâches de traitement de langage naturel.\n",
    "Critères de Sélection : on doit prendre en compte les capacités de traitement de langage du modèle, la facilité d'utilisation, la disponibilité des ressources (comme les modèles pré-entraînés), et les exigences matérielles ( si un GPU est nécessaire), le domaine, la langue.\n",
    "2. Évaluer les Performances du LLM sur un Ensemble de Test\n",
    "Préparation des Données : on doit divisez nos 3 corpus en ensembles d''entraînement, de validation et de test.\n",
    "Métriques de Performance : on peut utilisez des métriques telles que la précision, pour évaluer les performances initiales du LLM sur l'ensemble de test.\n",
    "3. Optimiser les Performances\n",
    "Ajustement de la Taille de l''Ensemble de Test : on doit testez différentes tailles pour l''ensemble de test et évaluez comment cela affecte les performances du modèle.\n",
    "4. Fine-Tuning du LLM Sélectionné\n",
    "Adaptation aux données Spécifiques : Fine-tunez le modèle sur notre ensemble d''entraînement pour améliorer ses performances sur les données de commentaires spécifiques.\n",
    "Ajustements des Hyperparamètres : On peut expérimentez avec différents taux d''apprentissage, tailles de batch, et autres hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8104c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# on lit les corpus 1, 2 et 3\n",
    "chemin_fichier_xml1 = 'C:\\\\Users\\\\docme\\\\Desktop\\\\MASTER_2_CHPS\\\\IA_LLM_KAMEL\\\\PROJET_AMIRA_MEHRI_LLM\\\\corpus_1.xml'\n",
    "chemin_fichier_xml2 = 'C:\\\\Users\\\\docme\\\\Desktop\\\\MASTER_2_CHPS\\\\IA_LLM_KAMEL\\\\PROJET_AMIRA_MEHRI_LLM\\\\corpus_2.xml'\n",
    "chemin_fichier_xml3 = 'C:\\\\Users\\\\docme\\\\Desktop\\\\MASTER_2_CHPS\\\\IA_LLM_KAMEL\\\\PROJET_AMIRA_MEHRI_LLM\\\\corpus_3.xml'\n",
    "\n",
    "# Parse les fichier XML\n",
    "tree1 = ET.parse(chemin_fichier_xml1)\n",
    "tree2 = ET.parse(chemin_fichier_xml2)\n",
    "tree3 = ET.parse(chemin_fichier_xml3)\n",
    "root1 = tree1.getroot()\n",
    "root2 = tree2.getroot()\n",
    "root3 = tree3.getroot()\n",
    "\n",
    "# Fonction 1 pour extraire les informations\n",
    "def extract_info(root1):\n",
    "    # Extraire les informations de la série et des commentaires\n",
    "    series_info1 = []\n",
    "    for serie_commentaires in root1.findall('SERIE_COMMENTAIRES'):\n",
    "        serie = serie_commentaires.find('SERIE').text\n",
    "        serie_id = serie_commentaires.find('SERIE').get('id')\n",
    "        comments = []\n",
    "        for commentaire in serie_commentaires.findall('COMMENTAIRES/COMMENTAIRE'):\n",
    "            comment_id = commentaire.get('id')\n",
    "            author = commentaire.get('auth')\n",
    "            evaluation = commentaire.get('eval')\n",
    "            text = commentaire.text\n",
    "            comments.append({\n",
    "                'id': comment_id,\n",
    "                'author': author,\n",
    "                'evaluation': evaluation,\n",
    "                'text': text\n",
    "            })\n",
    "        series_info1.append({\n",
    "            'serie': serie,\n",
    "            'serie_id': serie_id,\n",
    "            'comments': comments\n",
    "        })\n",
    "    return series_info1\n",
    "\n",
    "# Extraction des informations\n",
    "series_info1 = extract_info(root1)\n",
    "#print(series_info)  # Afficher les informations extraites\n",
    "print(series_info1[:10])  # Affiche seulement les 100 premiers éléments\n",
    "\n",
    "# Fonction 2 pour extraire les informations\n",
    "def extract_info(root2):\n",
    "    # Extraire les informations de la série et des commentaires\n",
    "    series_info2 = []\n",
    "    for serie_commentaires in root2.findall('SERIE_COMMENTAIRES'):\n",
    "        serie = serie_commentaires.find('SERIE').text\n",
    "        serie_id = serie_commentaires.find('SERIE').get('id')\n",
    "        comments = []\n",
    "        for commentaire in serie_commentaires.findall('COMMENTAIRES/COMMENTAIRE'):\n",
    "            comment_id = commentaire.get('id')\n",
    "            author = commentaire.get('auth')\n",
    "            evaluation = commentaire.get('eval')\n",
    "            text = commentaire.text\n",
    "            comments.append({\n",
    "                'id': comment_id,\n",
    "                'author': author,\n",
    "                'evaluation': evaluation,\n",
    "                'text': text\n",
    "            })\n",
    "        series_info2.append({\n",
    "            'serie': serie,\n",
    "            'serie_id': serie_id,\n",
    "            'comments': comments\n",
    "        })\n",
    "    return series_info2\n",
    "\n",
    "# Extraction des informations\n",
    "series_info2 = extract_info(root2)\n",
    "#print(series_info2)  # Afficher les informations extraites\n",
    "print(series_info2[:10])  # Affiche seulement les 100 premiers éléments\n",
    "\n",
    "# Fonction 2 pour extraire les informations\n",
    "def extract_info(root3):\n",
    "    # Extraire les informations de la série et des commentaires\n",
    "    series_info3 = []\n",
    "    for serie_commentaires in root3.findall('SERIE_COMMENTAIRES'):\n",
    "        serie = serie_commentaires.find('SERIE').text\n",
    "        serie_id = serie_commentaires.find('SERIE').get('id')\n",
    "        comments = []\n",
    "        for commentaire in serie_commentaires.findall('COMMENTAIRES/COMMENTAIRE'):\n",
    "            comment_id = commentaire.get('id')\n",
    "            author = commentaire.get('auth')\n",
    "            evaluation = commentaire.get('eval')\n",
    "            text = commentaire.text\n",
    "            comments.append({\n",
    "                'id': comment_id,\n",
    "                'author': author,\n",
    "                'evaluation': evaluation,\n",
    "                'text': text\n",
    "            })\n",
    "        series_info3.append({\n",
    "            'serie': serie,\n",
    "            'serie_id': serie_id,\n",
    "            'comments': comments\n",
    "        })\n",
    "    return series_info3\n",
    "\n",
    "# Extraction des informations\n",
    "series_info3 = extract_info(root3)\n",
    "#print(series_info2)  # Afficher les informations extraites\n",
    "print(series_info3[:10])  # Affiche seulement les 100 premiers éléments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_stars(score):\n",
    "    # Convertir le score en nombre flottant\n",
    "    score = float(score.replace(',', '.'))\n",
    "    # Attribuer des étoiles en fonction du score\n",
    "    stars = '★' * int(score)  # Étoile pleine pour chaque point entier\n",
    "    if score - int(score) >= 0.5:\n",
    "        stars += '½'  # Ajouter une demi-étoile si nécessaire\n",
    "    return stars\n",
    "\n",
    "# Parcourir chaque série et chaque commentaire pour transformer les scores\n",
    "for serie in series_info2:\n",
    "    for comment in serie['comments']:\n",
    "        score = comment['evaluation']\n",
    "        stars = score_to_stars(score)\n",
    "        print(f\"Série: {serie['serie']}, Commentaire ID: {comment['id']}, Score: {score}, Évaluation en étoiles: {stars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9718411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialiser le tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# On Assure que le tokenizer utilise le même token que le modèle pour le padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokeniser et encoder tous les commentaires\n",
    "encoded_comments = [tokenizer.encode_plus(\n",
    "    comment['text'],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ") for item in series_info2 for comment in item['comments']]\n",
    "\n",
    "# Maintenant, on doit extraire les input_ids et attention_masks\n",
    "input_ids = [ec['input_ids'][0] for ec in encoded_comments]\n",
    "attention_masks = [ec['attention_mask'][0] for ec in encoded_comments]\n",
    "\n",
    "# Cette opération suppose que encoded_comments est une liste de dictionnaires retournés par encode_plus\n",
    "input_ids = tf.concat([ec['input_ids'] for ec in encoded_comments], 0)\n",
    "attention_masks = tf.concat([ec['attention_mask'] for ec in encoded_comments], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ecd98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5 3.  5.  ... 5.  4.5 4.5], shape=(7706,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convertission de nos évaluations de string à float et remplacez les virgules par des points\n",
    "labels = tf.convert_to_tensor([float(comment['evaluation'].replace(',', '.')) for item in series_info2 for comment in item['comments']], dtype=tf.float32)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "# Convertission de nos tenseurs en numpy arrays \n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_masks_np = attention_masks.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Division de nos données\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids_np, labels_np, random_state=42, test_size=0.1)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks_np, labels_np, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0baa43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des inputs d'entraînement : (6935, 512)\n",
      "Forme des masques d'entraînement : (6935, 512)\n",
      "Forme des étiquettes d'entraînement : (6935,)\n",
      "Inputs d'entraînement, premiers éléments : [41840   235 41840   235 41840   235 41840   235 41840   235]\n",
      "Masques d'entraînement, premiers éléments : [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier la forme des données\n",
    "print(\"Forme des inputs d'entraînement :\", train_inputs.shape)\n",
    "print(\"Forme des masques d'entraînement :\", train_masks.shape)\n",
    "print(\"Forme des étiquettes d'entraînement :\", train_labels.shape)\n",
    "\n",
    "# Vérifier que les tensors ne contiennent pas que des zéros, ce qui indiquerait un problème de padding\n",
    "print(\"Inputs d'entraînement, premiers éléments :\", train_inputs[0][:10])\n",
    "print(\"Masques d'entraînement, premiers éléments :\", train_masks[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64c59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tfgpt2_model (TFGPT2Model)  TFBaseModelOutputWithPastA   1244398   ['input_ids[0][0]',           \n",
      "                             ndCrossAttentions(last_hid   08         'attention_mask[0][0]']      \n",
      "                             den_state=(None, 512, 768)                                           \n",
      "                             , past_key_values=((2, Non                                           \n",
      "                             e, 12, 512, 64),                                                     \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64)),                                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None, cross_attentio                                           \n",
      "                             ns=None)                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tfgpt2_model[0][0]']        \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 768)                  0         ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    769       ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124440577 (474.70 MB)\n",
      "Trainable params: 124440577 (474.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2Model, GPT2Tokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.regularizers import l2  # Importer la régularisation L2\n",
    "\n",
    "# Chargement du modèle GPT-2 pré-entraîné\n",
    "gpt2_model = TFGPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Définition des couches d'entrée pour les IDs de tokens et les masques d'attention\n",
    "input_ids_layer = Input(shape=(512,), dtype='int32', name='input_ids')\n",
    "attention_masks_layer = Input(shape=(512,), dtype='int32', name='attention_mask')\n",
    "\n",
    "# Obtontion des sorties du modèle GPT-2\n",
    "outputs = gpt2_model(input_ids_layer, attention_mask=attention_masks_layer)\n",
    "\n",
    "# GPT-2 n'a pas de 'pooler_output', donc nous utilisons directement les 'logits'\n",
    "# qui sont les scores avant l'activation finale\n",
    "# Prendre le dernier état caché pour représenter la séquence entière\n",
    "sequence_output = outputs.last_hidden_state\n",
    "\n",
    "# nous pouvons prendre le dernier token pour la classification\n",
    "# L'indexation dépend de la direction de l'attention dans le modèle GPT-2 utilisé\n",
    "clf_output = sequence_output[:, -1, :]  # Prendre le vecteur associé au dernier token\n",
    "\n",
    "# Appliquer le dropout pour la régularisation\n",
    "clf_output = Dropout(0.1)(clf_output)\n",
    "\n",
    "# Ajouter une couche dense pour la classification finale avec régularisation L2\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(clf_output)  # Pour la classification binaire\n",
    "\n",
    "# Construction du le modèle\n",
    "classification_model = Model(inputs=[input_ids_layer, attention_masks_layer], outputs=output_layer)\n",
    "\n",
    "# Compilation du modèle\n",
    "classification_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher le résumé du modèle\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69423e",
   "metadata": {},
   "source": [
    "Ce résumé nous indiques ces informations importantes : Entrée du Modèle:\n",
    "\n",
    "* input_ids: Entrée pour les identifiants des tokens, avec une forme de (None, 512). Cela indique que le modèle attend une séquence de 512 tokens.\n",
    "* attention_mask: Entrée pour les masques d'attention, également avec une forme de (None, 512).\n",
    "\n",
    "* Layer GPT-2 (TFGPT2Model):\n",
    "\n",
    " - Le modèle GPT-2 est utilisé comme couche principale. Il prend input_ids et attention_mask comme entrées.\n",
    " - Cette couche produit un last_hidden_state de taille (None, 512, 768), où 512 est la longueur de la séquence et 768 est la taille des vecteurs de caractéristiques.\n",
    "\n",
    "* Traitement Post-Modèle: \n",
    "       - tf.__operators__.getitem: Cette opération extrait le dernier état caché associé au dernier token (index -1) pour chaque séquence. La sortie de cette couche est de taille (None, 768).\n",
    "       - dropout_37: Applique un dropout pour la régularisation, avec un taux de 0.1.\n",
    "\n",
    "* Couche de Classification: Une couche dense avec une seule unité (None, 1) et une fonction d'activation 'sigmoid'. Cela suggère que votre modèle est configuré pour une classification binaire.\n",
    "* Paramètres: - Le modèle a un total de 124,440,577 paramètres, tous étant entraînables. Cela reflète la taille et la complexité du modèle GPT-2.\n",
    "              - Compilation: Le modèle est compilé avec une optimisation Adam et une perte de 'categorical_crossentropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35a633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\docme\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867/867 [==============================] - 11911s 14s/step - loss: 0.0392 - accuracy: 0.0239 - val_loss: 0.0641 - val_accuracy: 0.0246\n",
      "25/25 [==============================] - 440s 18s/step - loss: 0.0641 - accuracy: 0.0246\n",
      "Test Loss: 0.06412836909294128, Test Accuracy: 0.024643320590257645\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "# Ajuste `batch_size` et `epochs` selon mon pc  et les besoins du modèle\n",
    "history = classification_model.fit(\n",
    "    [train_inputs, train_masks],\n",
    "    train_labels,\n",
    "    validation_data=([test_inputs, test_masks], test_labels),\n",
    "    batch_size=8,  # Taille du batch, peut être réduite ou augmentée selon la capacité de votre machine\n",
    "    epochs=1,  # Nombre d'époques, à ajuster selon la convergence du modèle\n",
    "    verbose=1  # Pour afficher la progression de l'entraînement, mettre à 2 pour moins de détails\n",
    ")\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "test_loss, test_accuracy = classification_model.evaluate(\n",
    "    [test_inputs, test_masks],\n",
    "    test_labels,\n",
    "    verbose=1\n",
    ")\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa90697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 58/434 [===>..........................] - ETA: 3:48:04 - loss: 0.0840 - accuracy: 0.0216"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tune le modèle sur notre ensemble de données\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_masks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Données d'entraînement\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Étiquettes d'entraînement\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_masks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Données de validation\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Vous pouvez essayer d'ajuster la taille du batch\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Et le nombre d'époques\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Pour afficher la progression de l'entraînement\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Sauvegarder le modèle après l'entraînement\u001b[39;00m\n\u001b[0;32m     12\u001b[0m classification_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_save_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune le modèle sur notre ensemble de données\n",
    "history = classification_model.fit(\n",
    "    [train_inputs, train_masks],  # Données d'entraînement\n",
    "    train_labels,                 # Étiquettes d'entraînement\n",
    "    validation_data=([test_inputs, test_masks], test_labels),  # Données de validation\n",
    "    batch_size=16,                # on peut essayer d'ajuster la taille du batch\n",
    "    epochs=2,                     # Et le nombre d'époques\n",
    "    verbose=1                     # Pour afficher la progression de l'entraînement\n",
    ")\n",
    "\n",
    "# Sauvegarder le modèle après l'entraînement\n",
    "classification_model.save('path_to_save_model')\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "test_loss, test_accuracy = classification_model.evaluate(\n",
    "    [test_inputs, test_masks],  # Données de test\n",
    "    \n",
    "    test_labels,                # Étiquettes de test\n",
    "    verbose=1                   # Pour afficher les résultats de l'évaluation\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# nous pouvons également tracer l'historique de l'entraînement pour visualiser les performances du modèle au fil des époques\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723033e",
   "metadata": {},
   "source": [
    "Mon PC n'a pas pu terminer la phase de fine tuning, il est même bloqué donc j'ai arrêté l'exécution du noyau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa513340",
   "metadata": {},
   "source": [
    "J''ai choisis le modèle GPT2 car il est puissant il travaille sur plusieurs domaines, il fonctionne de manière auto-régressive, générant un token à la fois et utilisant les tokens précédemment générés comme contexte pour la génération du token suivant. Aussi GPT se base exclusivement sur la partie décodeur du Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e970e12",
   "metadata": {},
   "source": [
    "J'' ai essayé un seul corpus, car je n''ai pas un ordinateur plus puisssaant pour l''entrainement des modèles, j''ai créé la fonction score_to_stars qui convertit les commentaires en des étoiles mais j''ai eu le temps de l'utiliser dans mon modèle.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316bf46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
