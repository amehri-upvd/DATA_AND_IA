{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76031b72",
   "metadata": {},
   "source": [
    "# Projet Traitement de données multidimensionnelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c38824",
   "metadata": {},
   "source": [
    "Le projet vise à créer une intelligence artificielle (IA) utilisant un Large Language Model (LLM) pour catégoriser automatiquement les commentaires sur des séries télévisées. L'approche détaillée pour réaliser ce projet :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a235dd",
   "metadata": {},
   "source": [
    "1. Choixd'un LLM adéquat\n",
    "Il y a plusieursmodèles comme BERT, GPT-2 ou GPT-3..., chaque modèle a ses forces spécifiques pour différentes tâches de traitement de langage naturel.\n",
    "Critères de Sélection : on doit prendre en compte les capacités de traitement de langage du modèle, la facilité d'utilisation, la disponibilité des ressources (comme les modèles pré-entraînés), et les exigences matérielles ( si un GPU est nécessaire), le domaine, la langue.\n",
    "2. Évaluer les Performances du LLM sur un Ensemble de Test\n",
    "Préparation des Données : on doit divisez nos 3 corpus en ensembles d''entraînement, de validation et de test.\n",
    "Métriques de Performance : on peut utilisez des métriques telles que la précision, pour évaluer les performances initiales du LLM sur l'ensemble de test.\n",
    "3. Optimiser les Performances\n",
    "Ajustement de la Taille de l''Ensemble de Test : on doit testez différentes tailles pour l''ensemble de test et évaluez comment cela affecte les performances du modèle.\n",
    "4. Fine-Tuning du LLM Sélectionné\n",
    "Adaptation aux données Spécifiques : Fine-tunez le modèle sur notre ensemble d''entraînement pour améliorer ses performances sur les données de commentaires spécifiques.\n",
    "Ajustements des Hyperparamètres : On peut expérimentez avec différents taux d''apprentissage, tailles de batch, et autres hyperparamètres.\n",
    "5. Évaluation des Performances Après Fine-Tuning\n",
    "Tests Post Fine-Tuning : Après le fine-tuning, on peut réévaluez les performances du modèle sur notre ensemble de test.\n",
    "Comparaison Avant/Après : on peut comparez les performances avant et après le fine-tuning pour mesurer les améliorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8104c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# on lit les corpus 1, 2 et 3\n",
    "chemin_fichier_xml1 = 'C:\\\\Users\\\\docme\\\\Desktop\\\\MASTER_2_CHPS\\\\IA_LLM_KAMEL\\\\PROJET_AMIRA_MEHRI_LLM\\\\corpus_1.xml'\n",
    "chemin_fichier_xml2 = 'C:\\\\Users\\\\docme\\\\Desktop\\\\MASTER_2_CHPS\\\\IA_LLM_KAMEL\\\\PROJET_AMIRA_MEHRI_LLM\\\\corpus_2.xml'\n",
    "chemin_fichier_xml3 = 'C:\\\\Users\\\\docme\\\\Desktop\\\\MASTER_2_CHPS\\\\IA_LLM_KAMEL\\\\PROJET_AMIRA_MEHRI_LLM\\\\corpus_3.xml'\n",
    "\n",
    "# Parse les fichier XML\n",
    "tree1 = ET.parse(chemin_fichier_xml1)\n",
    "tree2 = ET.parse(chemin_fichier_xml2)\n",
    "tree3 = ET.parse(chemin_fichier_xml3)\n",
    "root1 = tree1.getroot()\n",
    "root2 = tree2.getroot()\n",
    "root3 = tree3.getroot()\n",
    "\n",
    "# Fonction 1 pour extraire les informations\n",
    "def extract_info(root1):\n",
    "    # Extraire les informations de la série et des commentaires\n",
    "    series_info1 = []\n",
    "    for serie_commentaires in root1.findall('SERIE_COMMENTAIRES'):\n",
    "        serie = serie_commentaires.find('SERIE').text\n",
    "        serie_id = serie_commentaires.find('SERIE').get('id')\n",
    "        comments = []\n",
    "        for commentaire in serie_commentaires.findall('COMMENTAIRES/COMMENTAIRE'):\n",
    "            comment_id = commentaire.get('id')\n",
    "            author = commentaire.get('auth')\n",
    "            evaluation = commentaire.get('eval')\n",
    "            text = commentaire.text\n",
    "            comments.append({\n",
    "                'id': comment_id,\n",
    "                'author': author,\n",
    "                'evaluation': evaluation,\n",
    "                'text': text\n",
    "            })\n",
    "        series_info1.append({\n",
    "            'serie': serie,\n",
    "            'serie_id': serie_id,\n",
    "            'comments': comments\n",
    "        })\n",
    "    return series_info1\n",
    "\n",
    "# Extraction des informations\n",
    "series_info1 = extract_info(root1)\n",
    "#print(series_info)  # Afficher les informations extraites\n",
    "print(series_info1[:10])  # Affiche seulement les 100 premiers éléments\n",
    "\n",
    "# Fonction 2 pour extraire les informations\n",
    "def extract_info(root2):\n",
    "    # Extraire les informations de la série et des commentaires\n",
    "    series_info2 = []\n",
    "    for serie_commentaires in root2.findall('SERIE_COMMENTAIRES'):\n",
    "        serie = serie_commentaires.find('SERIE').text\n",
    "        serie_id = serie_commentaires.find('SERIE').get('id')\n",
    "        comments = []\n",
    "        for commentaire in serie_commentaires.findall('COMMENTAIRES/COMMENTAIRE'):\n",
    "            comment_id = commentaire.get('id')\n",
    "            author = commentaire.get('auth')\n",
    "            evaluation = commentaire.get('eval')\n",
    "            text = commentaire.text\n",
    "            comments.append({\n",
    "                'id': comment_id,\n",
    "                'author': author,\n",
    "                'evaluation': evaluation,\n",
    "                'text': text\n",
    "            })\n",
    "        series_info2.append({\n",
    "            'serie': serie,\n",
    "            'serie_id': serie_id,\n",
    "            'comments': comments\n",
    "        })\n",
    "    return series_info2\n",
    "\n",
    "# Extraction des informations\n",
    "series_info2 = extract_info(root2)\n",
    "#print(series_info2)  # Afficher les informations extraites\n",
    "print(series_info2[:10])  # Affiche seulement les 100 premiers éléments\n",
    "\n",
    "# Fonction 2 pour extraire les informations\n",
    "def extract_info(root3):\n",
    "    # Extraire les informations de la série et des commentaires\n",
    "    series_info3 = []\n",
    "    for serie_commentaires in root3.findall('SERIE_COMMENTAIRES'):\n",
    "        serie = serie_commentaires.find('SERIE').text\n",
    "        serie_id = serie_commentaires.find('SERIE').get('id')\n",
    "        comments = []\n",
    "        for commentaire in serie_commentaires.findall('COMMENTAIRES/COMMENTAIRE'):\n",
    "            comment_id = commentaire.get('id')\n",
    "            author = commentaire.get('auth')\n",
    "            evaluation = commentaire.get('eval')\n",
    "            text = commentaire.text\n",
    "            comments.append({\n",
    "                'id': comment_id,\n",
    "                'author': author,\n",
    "                'evaluation': evaluation,\n",
    "                'text': text\n",
    "            })\n",
    "        series_info3.append({\n",
    "            'serie': serie,\n",
    "            'serie_id': serie_id,\n",
    "            'comments': comments\n",
    "        })\n",
    "    return series_info3\n",
    "\n",
    "# Extraction des informations\n",
    "series_info3 = extract_info(root3)\n",
    "#print(series_info2)  # Afficher les informations extraites\n",
    "print(series_info3[:10])  # Affiche seulement les 100 premiers éléments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_stars(score):\n",
    "    # Convertir le score en nombre flottant\n",
    "    score = float(score.replace(',', '.'))\n",
    "    # Attribuer des étoiles en fonction du score\n",
    "    stars = '★' * int(score)  # Étoile pleine pour chaque point entier\n",
    "    if score - int(score) >= 0.5:\n",
    "        stars += '½'  # Ajouter une demi-étoile si nécessaire\n",
    "    return stars\n",
    "\n",
    "# Parcourir chaque série et chaque commentaire pour transformer les scores\n",
    "for serie in series_info2:\n",
    "    for comment in serie['comments']:\n",
    "        score = comment['evaluation']\n",
    "        stars = score_to_stars(score)\n",
    "        print(f\"Série: {serie['serie']}, Commentaire ID: {comment['id']}, Score: {score}, Évaluation en étoiles: {stars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9718411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialiser le tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# On Assure que le tokenizer utilise le même token que le modèle pour le padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokeniser et encoder tous les commentaires\n",
    "encoded_comments = [tokenizer.encode_plus(\n",
    "    comment['text'],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ") for item in series_info2 for comment in item['comments']]\n",
    "\n",
    "# Maintenant, on doit extraire les input_ids et attention_masks\n",
    "input_ids = [ec['input_ids'][0] for ec in encoded_comments]\n",
    "attention_masks = [ec['attention_mask'][0] for ec in encoded_comments]\n",
    "\n",
    "# Cette opération suppose que encoded_comments est une liste de dictionnaires retournés par encode_plus\n",
    "input_ids = tf.concat([ec['input_ids'] for ec in encoded_comments], 0)\n",
    "attention_masks = tf.concat([ec['attention_mask'] for ec in encoded_comments], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ecd98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5 3.  5.  ... 5.  4.5 4.5], shape=(7706,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convertissez vos évaluations de string à float et remplacez les virgules par des points\n",
    "labels = tf.convert_to_tensor([float(comment['evaluation'].replace(',', '.')) for item in series_info2 for comment in item['comments']], dtype=tf.float32)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "# Convertissez vos tenseurs en numpy arrays si nécessaire\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_masks_np = attention_masks.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Divisez les données\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids_np, labels_np, random_state=42, test_size=0.1)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks_np, labels_np, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0baa43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des inputs d'entraînement : (6935, 512)\n",
      "Forme des masques d'entraînement : (6935, 512)\n",
      "Forme des étiquettes d'entraînement : (6935,)\n",
      "Inputs d'entraînement, premiers éléments : [41840   235 41840   235 41840   235 41840   235 41840   235]\n",
      "Masques d'entraînement, premiers éléments : [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier la forme des données\n",
    "print(\"Forme des inputs d'entraînement :\", train_inputs.shape)\n",
    "print(\"Forme des masques d'entraînement :\", train_masks.shape)\n",
    "print(\"Forme des étiquettes d'entraînement :\", train_labels.shape)\n",
    "\n",
    "# Vérifier que les tensors ne contiennent pas que des zéros, ce qui indiquerait un problème de padding\n",
    "print(\"Inputs d'entraînement, premiers éléments :\", train_inputs[0][:10])\n",
    "print(\"Masques d'entraînement, premiers éléments :\", train_masks[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64c59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tfgpt2_model (TFGPT2Model)  TFBaseModelOutputWithPastA   1244398   ['input_ids[0][0]',           \n",
      "                             ndCrossAttentions(last_hid   08         'attention_mask[0][0]']      \n",
      "                             den_state=(None, 512, 768)                                           \n",
      "                             , past_key_values=((2, Non                                           \n",
      "                             e, 12, 512, 64),                                                     \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64),                                             \n",
      "                              (2, None, 12, 512, 64)),                                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None, cross_attentio                                           \n",
      "                             ns=None)                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tfgpt2_model[0][0]']        \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 768)                  0         ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    769       ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124440577 (474.70 MB)\n",
      "Trainable params: 124440577 (474.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2Model, GPT2Tokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.regularizers import l2  # Importer la régularisation L2\n",
    "\n",
    "# Charger le modèle GPT-2 pré-entraîné\n",
    "gpt2_model = TFGPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Définir les couches d'entrée pour les IDs de tokens et les masques d'attention\n",
    "input_ids_layer = Input(shape=(512,), dtype='int32', name='input_ids')\n",
    "attention_masks_layer = Input(shape=(512,), dtype='int32', name='attention_mask')\n",
    "\n",
    "# Obtenir les sorties du modèle GPT-2\n",
    "outputs = gpt2_model(input_ids_layer, attention_mask=attention_masks_layer)\n",
    "\n",
    "# GPT-2 n'a pas de 'pooler_output', donc nous utilisons directement les 'logits'\n",
    "# qui sont les scores avant l'activation finale\n",
    "# Prendre le dernier état caché pour représenter la séquence entière\n",
    "sequence_output = outputs.last_hidden_state\n",
    "\n",
    "# Vous pouvez prendre le dernier token pour la classification\n",
    "# Notez que l'indexation dépend de la direction de l'attention dans le modèle GPT-2 utilisé\n",
    "clf_output = sequence_output[:, -1, :]  # Prendre le vecteur associé au dernier token\n",
    "\n",
    "# Appliquer le dropout pour la régularisation\n",
    "clf_output = Dropout(0.1)(clf_output)\n",
    "\n",
    "# Ajouter une couche dense pour la classification finale avec régularisation L2\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(clf_output)  # Pour la classification binaire\n",
    "\n",
    "# Construire le modèle\n",
    "classification_model = Model(inputs=[input_ids_layer, attention_masks_layer], outputs=output_layer)\n",
    "\n",
    "# Compiler le modèle\n",
    "classification_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher le résumé du modèle\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69423e",
   "metadata": {},
   "source": [
    "Le résumé montre les différentes couches de notre modèle, \n",
    "y compris les couches d''entrée, le modèle GPT-2, la couche de dropout, et la couche de sortie dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\docme\\anaconda3\\envs\\monPython\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867/867 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.0239 "
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "# Ajuste `batch_size` et `epochs` selon mon pc  et les besoins du modèle\n",
    "history = classification_model.fit(\n",
    "    [train_inputs, train_masks],\n",
    "    train_labels,\n",
    "    validation_data=([test_inputs, test_masks], test_labels),\n",
    "    batch_size=8,  # Taille du batch, peut être réduite ou augmentée selon la capacité de votre machine\n",
    "    epochs=1,  # Nombre d'époques, à ajuster selon la convergence du modèle\n",
    "    verbose=1  # Pour afficher la progression de l'entraînement, mettre à 2 pour moins de détails\n",
    ")\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "test_loss, test_accuracy = classification_model.evaluate(\n",
    "    [test_inputs, test_masks],\n",
    "    test_labels,\n",
    "    verbose=1\n",
    ")\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa90697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune le modèle sur votre ensemble de données\n",
    "history = classification_model.fit(\n",
    "    [train_inputs, train_masks],  # Données d'entraînement\n",
    "    train_labels,                 # Étiquettes d'entraînement\n",
    "    validation_data=([test_inputs, test_masks], test_labels),  # Données de validation\n",
    "    batch_size=16,                # Vous pouvez essayer d'ajuster la taille du batch\n",
    "    epochs=4,                     # Et le nombre d'époques\n",
    "    verbose=1                     # Pour afficher la progression de l'entraînement\n",
    ")\n",
    "\n",
    "# Sauvegarder le modèle après l'entraînement\n",
    "classification_model.save('path_to_save_model')\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "test_loss, test_accuracy = classification_model.evaluate(\n",
    "    [test_inputs, test_masks],  # Données de test\n",
    "    test_labels,                # Étiquettes de test\n",
    "    verbose=1                   # Pour afficher les résultats de l'évaluation\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# nous pouvons également tracer l'historique de l'entraînement pour visualiser les performances du modèle au fil des époques\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
